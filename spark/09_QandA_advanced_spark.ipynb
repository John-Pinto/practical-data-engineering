{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888fa5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be4e0ea-9f0c-49a7-899b-b0329a362775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Advanced Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a17efe6-ceb3-4cec-9356-95866383f1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- scrape_id: long (nullable = true)\n",
      " |-- last_scraped: date (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- neighborhood_overview: string (nullable = true)\n",
      " |-- picture_url: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_url: string (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- host_since: date (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_about: string (nullable = true)\n",
      " |-- host_response_time: string (nullable = true)\n",
      " |-- host_response_rate: string (nullable = true)\n",
      " |-- host_acceptance_rate: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_thumbnail_url: string (nullable = true)\n",
      " |-- host_picture_url: string (nullable = true)\n",
      " |-- host_neighbourhood: string (nullable = true)\n",
      " |-- host_listings_count: integer (nullable = true)\n",
      " |-- host_total_listings_count: integer (nullable = true)\n",
      " |-- host_verifications: string (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- neighbourhood_cleansed: string (nullable = true)\n",
      " |-- neighbourhood_group_cleansed: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- bathrooms_text: string (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- amenities: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- maximum_nights: integer (nullable = true)\n",
      " |-- minimum_minimum_nights: integer (nullable = true)\n",
      " |-- maximum_minimum_nights: integer (nullable = true)\n",
      " |-- minimum_maximum_nights: integer (nullable = true)\n",
      " |-- maximum_maximum_nights: integer (nullable = true)\n",
      " |-- minimum_nights_avg_ntm: double (nullable = true)\n",
      " |-- maximum_nights_avg_ntm: double (nullable = true)\n",
      " |-- calendar_updated: string (nullable = true)\n",
      " |-- has_availability: string (nullable = true)\n",
      " |-- availability_30: integer (nullable = true)\n",
      " |-- availability_60: integer (nullable = true)\n",
      " |-- availability_90: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- calendar_last_scraped: date (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- number_of_reviews_l30d: integer (nullable = true)\n",
      " |-- availability_eoy: integer (nullable = true)\n",
      " |-- number_of_reviews_ly: integer (nullable = true)\n",
      " |-- estimated_occupancy_l365d: integer (nullable = true)\n",
      " |-- estimated_revenue_l365d: integer (nullable = true)\n",
      " |-- first_review: date (nullable = true)\n",
      " |-- last_review: date (nullable = true)\n",
      " |-- review_scores_rating: double (nullable = true)\n",
      " |-- review_scores_accuracy: double (nullable = true)\n",
      " |-- review_scores_cleanliness: double (nullable = true)\n",
      " |-- review_scores_checkin: double (nullable = true)\n",
      " |-- review_scores_communication: double (nullable = true)\n",
      " |-- review_scores_location: double (nullable = true)\n",
      " |-- review_scores_value: double (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- instant_bookable: string (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_entire_homes: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_private_rooms: integer (nullable = true)\n",
      " |-- calculated_host_listings_count_shared_rooms: integer (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- listing_id: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- reviewer_id: integer (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_df = spark.read.csv(\n",
    "    path=\"./data/airbnb-london-listings.csv.gz\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\",\n",
    ")\n",
    "listings_df.printSchema()\n",
    "\n",
    "reviews_df = spark.read.csv(\n",
    "    path=\"./data/airbnb-london-reviews.csv.gz\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    sep=\",\",\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    multiLine=True,\n",
    "    mode=\"PERMISSIVE\",\n",
    ")\n",
    "reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51401f1-d6b2-4030-8b6d-2ae6af6494f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. For each listing compute string category depending on its price, and add it as a new column.\n",
    "# A category is defined in the following way:\n",
    "#\n",
    "# * price < 50 -> \"Budget\"\n",
    "# * 50 <= price < 150 -> \"Mid-range\"\n",
    "# * price >= 150 -> \"Luxury\"\n",
    "#\n",
    "# Only include listings where the price is not null.\n",
    "# Count the number of listings in each category\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "listings_df = listings_df.withColumn(\n",
    "    \"price_numeric\", regexp_replace(\"price\", \"[$,]\", \"\").cast(\"float\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5092f446-69d4-4e24-825e-98c3a4e33884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_category(price):\n",
    "    if price < 50:\n",
    "        return \"Budget\"\n",
    "    elif 50 <= price < 150:\n",
    "        return \"Mid-range\"\n",
    "    elif price >= 150:\n",
    "        return \"Luxury\"\n",
    "    elif price is None:\n",
    "        return \"Unknown\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "price_category_udf = udf(price_category, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "776853ac-90a7-443a-814e-ecde221c79eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|price_category|count|\n",
      "+--------------+-----+\n",
      "|     Mid-range|28333|\n",
      "|        Budget| 6114|\n",
      "|        Luxury|27516|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listings_with_category = (\n",
    "    listings_df.filter(listings_df.price_numeric.isNotNull())\n",
    "    .withColumn(\"price_category\", price_category_udf(listings_df.price_numeric))\n",
    "    .groupby(\"price_category\")\n",
    "    .count()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd77ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------------------------------+-------------------+\n",
      "|id                 |name                                              |avg_sentiment_score|\n",
      "+-------------------+--------------------------------------------------+-------------------+\n",
      "|8630729            |Central and Cozy 2 BR Flat next to Pimlico station|6.0                |\n",
      "|1475288755040843693|Quiet room 10 mins walk to portobello rd          |6.0                |\n",
      "|945592509209998667 |Cozy One Bedroom Full Flat                        |5.0                |\n",
      "|24763465           |Luxury Holiday Let | Prime SW19 Village Location  |5.0                |\n",
      "|3804150            |London NW3. Feel at home single room              |5.0                |\n",
      "|1366557021812675836|Fulham Townhouse                                  |5.0                |\n",
      "|39575614           |Light-filled 1 bed Victorian flat in Canonbury    |5.0                |\n",
      "|1328328005396938099|Private Room in cosy house                        |5.0                |\n",
      "|51832910           |Luxury Modern Spacious-Great Connections- Parking |5.0                |\n",
      "|15488286           |Canal Side stylish 1 Bedroom Apartment near tube  |5.0                |\n",
      "|1312322528229369543|Designer New Built 1bed Flat close to Hampstead   |5.0                |\n",
      "|34277415           |Gorgeous family home in Fulham for 6 by the river |5.0                |\n",
      "|1255822576651828200|Beautiful Victorian Terrace Home                  |5.0                |\n",
      "|40386257           |Beautifully Contemporary Three Bedroom House      |5.0                |\n",
      "|1179493457055839061|Modern cozy room with p.bathroom                  |5.0                |\n",
      "|41336114           |Charming Period one Bedroom Garden Apartment      |5.0                |\n",
      "|1300600656895202732|Bright&cosy flat in Kensal Rise                   |5.0                |\n",
      "|1479062711716406004|Elegant Marylebone Home                           |5.0                |\n",
      "|1484006101384799196|Luxury 2 Bed Duplex in Chelsea                    |5.0                |\n",
      "|1202038991295609757|Teddington Lock. Spacious Family Home             |5.0                |\n",
      "+-------------------+--------------------------------------------------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# 2. In this task you will need to compute a sentiment score per review, and then an average sentiment score per listing.\n",
    "# A sentiment score indicates how \"positive\" or \"negative\" a review is. The higher the score the more positive it is, and vice-versa.\n",
    "#\n",
    "# To compute a sentiment score per review compute the number of positive words in a review and subtract the number of negative\n",
    "# words in the same review (the list of words is already provided)\n",
    "#\n",
    "# To complete this task, compute a DataFrame that contains the following fields:\n",
    "# * name - the name of a listing\n",
    "# * average_sentiment - average sentiment of reviews computed using the algorithm described above\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Lists of positive and negative words\n",
    "positive_words = {\n",
    "    \"good\",\n",
    "    \"great\",\n",
    "    \"excellent\",\n",
    "    \"amazing\",\n",
    "    \"fantastic\",\n",
    "    \"wonderful\",\n",
    "    \"pleasant\",\n",
    "    \"lovely\",\n",
    "    \"nice\",\n",
    "    \"enjoyed\",\n",
    "}\n",
    "negative_words = {\n",
    "    \"bad\",\n",
    "    \"terrible\",\n",
    "    \"awful\",\n",
    "    \"horrible\",\n",
    "    \"disappointing\",\n",
    "    \"poor\",\n",
    "    \"hate\",\n",
    "    \"unpleasant\",\n",
    "    \"dirty\",\n",
    "    \"noisy\",\n",
    "}\n",
    "\n",
    "\n",
    "def sentiment_score(comments):\n",
    "    if comments is None:\n",
    "        return 0.0\n",
    "\n",
    "    comments_lower = comments.lower()\n",
    "    score = 0\n",
    "\n",
    "    for pos_word, neg_word in zip(positive_words, negative_words):\n",
    "        if pos_word in comments_lower:\n",
    "            score += 1\n",
    "\n",
    "        if neg_word in comments_lower:\n",
    "            score -= 1\n",
    "\n",
    "    return float(score)\n",
    "\n",
    "\n",
    "sentiment_score_udf = udf(sentiment_score, FloatType())\n",
    "\n",
    "reviews_with_sentiment = reviews_df.withColumn(\n",
    "    \"sentiment_score\",\n",
    "    sentiment_score_udf(reviews_df.comments),\n",
    ")\n",
    "\n",
    "listings_df.join(\n",
    "    other=reviews_with_sentiment,\n",
    "    on=listings_df.id == reviews_df.listing_id,\n",
    "    how=\"inner\",\n",
    ").groupBy(\n",
    "    listings_df.id,\n",
    "    listings_df.name,\n",
    ").agg(\n",
    "    avg(reviews_with_sentiment.sentiment_score).alias(\"avg_sentiment_score\"),\n",
    ").orderBy(\n",
    "    \"avg_sentiment_score\",\n",
    "    ascending=False,\n",
    ").show(\n",
    "    truncate=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193cf773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------------+\n",
      "|listing_id        |avg_comments_length|reviews_count|\n",
      "+------------------+-------------------+-------------+\n",
      "|618608352812465378|1300.1666666666667 |6            |\n",
      "|28508447          |1089.3333333333333 |6            |\n",
      "|22661311          |1035.857142857143  |7            |\n",
      "|53145228          |1006.6666666666666 |6            |\n",
      "|627425975703032358|951.7777777777778  |9            |\n",
      "|2197681           |939.2              |5            |\n",
      "|13891813          |905.0              |5            |\n",
      "|979753            |893.9230769230769  |13           |\n",
      "|630150178279666225|890.7272727272727  |11           |\n",
      "|8856894           |890.1666666666666  |6            |\n",
      "|33310686          |885.8333333333334  |6            |\n",
      "|22524075          |885.0              |5            |\n",
      "|29469389          |885.0              |6            |\n",
      "|5555679           |878.7169811320755  |106          |\n",
      "|6594477           |863.6              |5            |\n",
      "|33385444          |848.0              |5            |\n",
      "|565214            |834.0833333333334  |12           |\n",
      "|53493254          |831.0              |7            |\n",
      "|12646480          |819.6              |5            |\n",
      "|8574525           |813.6666666666666  |6            |\n",
      "+------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# 3. Rewrite the following code from the previous exercise using SparkSQL:\n",
    "#\n",
    "# ```\n",
    "# from pyspark.sql.functions import length, avg, count\n",
    "#\n",
    "# reviews_with_comment_length = reviews.withColumn('comment_length', length('comments'))\n",
    "# reviews_with_comment_length \\\n",
    "#   .join(listings, reviews_with_comment_length.listing_id == listings.id, 'inner') \\\n",
    "#   .groupBy('listing_id').agg(\n",
    "#       avg(reviews_with_comment_length.comment_length).alias('average_comment_length'),\n",
    "#       count(reviews_with_comment_length.id).alias('reviews_count')\n",
    "#   ) \\\n",
    "#   .filter('reviews_count >= 5') \\\n",
    "#   .orderBy('average_comment_length', ascending=False) \\\n",
    "#   .show()\n",
    "# ```\n",
    "# This was a solution for the the task:\n",
    "#\n",
    "# \"Get top five listings with the highest average review comment length. Only return listings with at least 5 reviews\"\n",
    "\n",
    "listings_df.createOrReplaceTempView(\"listings\")\n",
    "reviews_df.createOrReplaceTempView(\"reviews\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "        r.listing_id,\n",
    "        AVG(LENGTH(r.comments)) AS avg_comments_length,\n",
    "        COUNT(r.id) AS reviews_count\n",
    "    FROM \n",
    "        listings AS l\n",
    "    INNER JOIN \n",
    "        reviews AS r\n",
    "        ON l.id = r.listing_id\n",
    "    GROUP BY\n",
    "        1\n",
    "    HAVING\n",
    "        COUNT(r.id) >= 5\n",
    "    ORDER BY\n",
    "        2 DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sqlQuery=query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70f68f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\installed_software\\Apache Spark\\spark-4.1.1-bin-hadoop3\\python\\pyspark\\sql\\pandas\\functions.py:761: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------+\n",
      "|host_id|avg_days_from_first_review|\n",
      "+-------+--------------------------+\n",
      "|  60302|                    5870.0|\n",
      "| 199549|                    5604.0|\n",
      "| 211291|                    5602.0|\n",
      "| 157884|                    5599.0|\n",
      "| 212734|                    5592.0|\n",
      "| 155938|                    5562.0|\n",
      "|  41759|                    5541.0|\n",
      "|  54730|                    5450.0|\n",
      "| 379315|                    5420.0|\n",
      "|  67564|                    5415.0|\n",
      "| 466353|                    5387.0|\n",
      "| 428600|                    5379.0|\n",
      "| 502496|                    5359.0|\n",
      "| 536025|                    5357.0|\n",
      "| 264345|                    5345.0|\n",
      "| 432648|                    5332.0|\n",
      "| 181028|                    5329.0|\n",
      "| 182993|                    5318.0|\n",
      "| 134938|                    5311.0|\n",
      "| 554519|                    5310.0|\n",
      "+-------+--------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# 4. [Optional][Challenge]\n",
    "# Calculate an average time passed from the first review for each listing in the listings dataset.\n",
    "# To implmenet a custom aggregation function you would need to use \"pandas_udf\" function to write a custom aggregation function.\n",
    "#\n",
    "# Documentation about \"pandas_udf\": https://spark.apache.org/docs/3.4.2/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html\n",
    "#\n",
    "# To use \"pandas_udf\" you would need to install two additional dependencies in the virtual environment you use for PySpark:\n",
    "# Run these commands:\n",
    "# ```\n",
    "# pip install pandas\n",
    "# pip install pyarrow\n",
    "# ```\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import PandasUDFType, pandas_udf\n",
    "from pyspark.sql.types import DateType, DoubleType\n",
    "\n",
    "\n",
    "@pandas_udf(returnType=DoubleType(), functionType=PandasUDFType.GROUPED_AGG)\n",
    "def avg_days_from_first_review_udf(first_review_series: DateType):\n",
    "    today_date = pd.to_datetime(\"today\")\n",
    "    avg_listing_ages = (today_date - pd.to_datetime(first_review_series)).mean().days\n",
    "\n",
    "    if pd.isna(avg_listing_ages):\n",
    "        return None\n",
    "\n",
    "    return float(avg_listing_ages)\n",
    "\n",
    "\n",
    "listings_df.filter(\n",
    "    listings_df.first_review.isNotNull(),\n",
    ").groupBy(\n",
    "    \"host_id\",\n",
    ").agg(\n",
    "    avg_days_from_first_review_udf(\n",
    "        listings_df.first_review,\n",
    "    ).alias(\"avg_days_from_first_review\"),\n",
    ").orderBy(\n",
    "    \"avg_days_from_first_review\",\n",
    "    ascending=False,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36de698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d90b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
